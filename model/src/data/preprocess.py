"""Module to preprocess the raw data for machine learning models.

This module includes functions to clean, transform, and feature engineer the raw data file(s)
generated by `make_dataset.py`. The preprocessed data will be saved in the `data/processed/` 
directory with a specific file name. The module may also include splitting the data into 
training and validation sets.

Functions:
    remove_duplicates(df): Removes duplicate rows from a Pandas DataFrame.
    handle_missing_values(df): Fills missing values in a Pandas DataFrame using various methods.
    feature_engineer(df): Creates new features or transforms existing ones in a Pandas DataFrame.
    split_data(df): Splits a Pandas DataFrame into training and validation sets.

Example usage:
    remove_duplicates(df)
    handle_missing_values(df)
    feature_engineer(df)
    split_data(df)

"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from transit_data import BRTData, NTDData
import os

# --------------------------------------------
# HELPER FUNCTIONS
# --------------------------------------------

def make_average(df, name):
    series_average = df.mean(axis=1)
    series_average.rename(name, inplace=True)

    return series_average

def export_csv(df, name):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_dir = os.path.abspath(os.path.join(script_dir, "../.."))
    data_dir = os.path.join(project_dir, "data/processed")
    
    df.to_csv(os.path.join(data_dir, f'{name}.csv'), index=True)

# --------------------------------------------
# DATA PROCESSING FUNCTIONS
# --------------------------------------------

def process_brt_data(brt_data: BRTData):
    metrics = ['income', 'pop', 'age', 'house_married', 'house_nonfam', 'house_m_single', 'house_f_single', 'car', 'biz']
    series_list = []

    for metric in metrics:
        df = brt_data.get_data(metric)
        
        # Cleaning
        df.dropna(axis=1, how='all', inplace=True) # drop cols with all NaN's
        df = df.loc[:, ~(df < 0).any(axis=0)] # drop cols that feature any negative
        df = df.loc[:, df.eq(0).mean() <= 0.25] # only keep cols with 25% or fewer values that are still 0

        series_average = make_average(df, metric)
        series_list.append(series_average)

    return pd.concat(series_list, axis=1)


def process_ntd_data(ntd_data: NTDData, system: str):
    years = list(range(2013, 2021))
    curr_name = system.replace("_", " ")
    cols = ['Unlinked Passenger Trips', 'Primary UZA\n Population', 'UZA Population', 'Mode VOMS', 'VOMS', 'Annual Vehicle Revenue Miles', 'Vehicle Revenue Miles']
    res_df = pd.DataFrame(columns = cols, index = years)

    for year in years:
        df_raw = ntd_data.get_data(f'transit_data_{year}_filtered')
        
        if year in (2013, 2014):
            filtered_df = df_raw[df_raw["Urbanized Area"].str.contains(curr_name, na=False, case=False)]
        else: 
            filtered_df = df_raw[df_raw["City"].str.contains(curr_name, na=False, case=False)]

        # TODO: doesn't grab roaring fork properly
        
        df_year = filtered_df.loc[:, filtered_df.columns.isin(cols)] 

        # Populate res_df
        for col in cols:
            try:
                res_df.at[year, col] = df_year[col][0]
            except:
                res_df.at[year, col] = np.NaN

    res_df.rename(columns={'Primary UZA\n Population': 'Primary UZA Population'}, inplace=True)

    # Merge duplicate cols
    res_df['UZA Population'].update(res_df.pop('Primary UZA Population'))
    res_df['VOMS'].update(res_df.pop('Mode VOMS'))
    res_df['Vehicle Revenue Miles'].update(res_df.pop('Annual Vehicle Revenue Miles'))

    # Column name cleaning - lowercase and space replacement
    res_df.columns= res_df.columns.str.lower()
    res_df.columns = res_df.columns.str.replace(' ', '_')

    # Cleaning values - removing commas from float fields that are mis-cast
    res_df.replace(',','', regex=True, inplace=True)
    res_df = res_df.astype({'unlinked_passenger_trips':'float'})
    res_df = res_df.astype({'uza_population':'float'})
    res_df = res_df.astype({'vehicle_revenue_miles':'float'})

    # Adjusting values off by 3 orders of magnitude somehow
    mean = res_df.mean()
    std = res_df.std()

    res_df.loc[abs(res_df['unlinked_passenger_trips'] - mean.unlinked_passenger_trips) > std.unlinked_passenger_trips, 'unlinked_passenger_trips'] *= 1000
    res_df.loc[abs(res_df['vehicle_revenue_miles'] - mean.vehicle_revenue_miles) > std.vehicle_revenue_miles, 'vehicle_revenue_miles'] *= 1000

    # return the dataframe for a specific system
    return res_df

def process_data(brt_data: BRTData, ntd_data: NTDData, system: str):
    brt_df = process_brt_data(brt_data)
    ntd_df = process_ntd_data(ntd_data, system)

    # Merge the dfs
    merged_df = pd.concat([brt_df, ntd_df], axis=1)
    merged_df = merged_df.round(2)
    print(merged_df)

    # Export the df to a CSV
    export_csv(merged_df, system)

def main():
    # locations = ['cleveland', 'houston', 'kansas', 'richmond', 'indianapolis', 'eugene', 'albuquerque', 'aspen_westcliffe_glenwood_springs', 'fort_collins', 'hartford', 'grand_rapids', 'orlando', 'boston', 'los_angeles']
    locations = ['boston', 'kansas', 'aspen_westcliffe_glenwood_springs', 'richmond', 'eugene']
    ntd = NTDData()
    ntd.load_existing_data()
    
    for system in locations:
        brt = BRTData(system)
        brt.load_existing_data()
        process_data(brt, ntd, system)

if __name__ == "__main__":
    main()
